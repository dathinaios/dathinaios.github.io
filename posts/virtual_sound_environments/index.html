<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Virtual Sound Environments: Concepts and Methods | Dionysis Athinaios</title><meta name=keywords content="virtual sound environments,audio,research"><meta name=description content="Introduction This article presents concepts that are useful for the creation of virtual sound environments. Although in most cases it uses examples from the real world, the concepts should not be viewed as an attempt to categorise, analyse or model real soundscapes. Throughout, the focus is on the creation of perceptually clear and dynamic virtual sound environments that allude to the experience of sound spaces and events, much in the same way that this is done in video games."><meta name=author content="Dionysis Athinaios"><link rel=canonical href=https://dathinaios.github.io/posts/virtual_sound_environments/><link crossorigin=anonymous href=/assets/css/stylesheet.e8e629800b5a0c7e57e1a4b303cf41f138b042b0b36be02ce26cc77775ff5c37.css integrity="sha256-6OYpgAtaDH5X4aSzA89B8TiwQrCza+As4mzHd3X/XDc=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://dathinaios.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dathinaios.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dathinaios.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://dathinaios.github.io/apple-touch-icon.png><link rel=mask-icon href=https://dathinaios.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel=stylesheet><meta property="og:title" content="Virtual Sound Environments: Concepts and Methods"><meta property="og:description" content="Introduction This article presents concepts that are useful for the creation of virtual sound environments. Although in most cases it uses examples from the real world, the concepts should not be viewed as an attempt to categorise, analyse or model real soundscapes. Throughout, the focus is on the creation of perceptually clear and dynamic virtual sound environments that allude to the experience of sound spaces and events, much in the same way that this is done in video games."><meta property="og:type" content="article"><meta property="og:url" content="https://dathinaios.github.io/posts/virtual_sound_environments/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2014-09-05T00:00:00+00:00"><meta property="article:modified_time" content="2014-09-05T00:00:00+00:00"><meta property="og:site_name" content="Dionysis Athinaios"><meta name=twitter:card content="summary"><meta name=twitter:title content="Virtual Sound Environments: Concepts and Methods"><meta name=twitter:description content="Introduction This article presents concepts that are useful for the creation of virtual sound environments. Although in most cases it uses examples from the real world, the concepts should not be viewed as an attempt to categorise, analyse or model real soundscapes. Throughout, the focus is on the creation of perceptually clear and dynamic virtual sound environments that allude to the experience of sound spaces and events, much in the same way that this is done in video games."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://dathinaios.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Virtual Sound Environments: Concepts and Methods","item":"https://dathinaios.github.io/posts/virtual_sound_environments/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Virtual Sound Environments: Concepts and Methods","name":"Virtual Sound Environments: Concepts and Methods","description":"Introduction This article presents concepts that are useful for the creation of virtual sound environments. Although in most cases it uses examples from the real world, the concepts should not be viewed as an attempt to categorise, analyse or model real soundscapes. Throughout, the focus is on the creation of perceptually clear and dynamic virtual sound environments that allude to the experience of sound spaces and events, much in the same way that this is done in video games.","keywords":["virtual sound environments","audio","research"],"articleBody":"Introduction This article presents concepts that are useful for the creation of virtual sound environments. Although in most cases it uses examples from the real world, the concepts should not be viewed as an attempt to categorise, analyse or model real soundscapes. Throughout, the focus is on the creation of perceptually clear and dynamic virtual sound environments that allude to the experience of sound spaces and events, much in the same way that this is done in video games.\nIn a video game, the creators do not attempt in most cases to recreate a real space. Their target is to create a self contained world, with its own rules, that would allow the player to engage and interact with its elements. There will be no consideration of the cultural and personal ramifications of the term soundscape that have been proposed during the past years 1. Instead a collection of concepts that allows for multiple interpretations of a sound environment depending on the intended outcome will be presented, leaving the decisions regarding content and its related meaning to the judgement of the artist or the analytical approach of the researcher accordingly.\nThe unit is defined as the basic element for the construction of a virtual sound environment and its several types, followed by considerations of some complimentary elements and a discussion concerning interaction. The second part of the article will touch on the basics of unit creation, as well as their behaviour and interaction, presenting a number of techniques that are essential for every implementation.\nThe Main Building Blocks Units A unit is a sound that can be mentally isolated from the environment for consideration and that perceptually has the capacity to stand as an autonomous entity. It is important to understand that the concept of a unit is based on a subjective interpretation of the world. Is the unit the swarm of bees? The individual bee? A cell in the bee’s body? Should the flowers be included as well? The boundaries of the world are judged by the human mind and can have multiple, sometimes conflicting, but nevertheless valid interpretations.\nSo how can a decision for what a unit is be made? On the simplest level, a unit seems to be described by a word. The fact that there is a symbol assigned to it is a clear indication that it has been identified as a unit. Taking a restaurant as an example there the following sounds can be heard: people talking, the sound of cutlery, the waiters footsteps as they go about their work, the gentle sound of their clothing as people move, music playing in the background. Occasionally a more distinct event appears such as the sound of pouring liquid in a glass, the ‘plop’ of a cork being removed or the sound of someone laughing.\nThis description could be chosen as a guide and the composition of a soundscape commence but that would not necessarily be the best strategy. In choosing the units that will be used, the amount of interactivity that the environment has is chosen as well. If a unit ‘crowd’ is placed in the background of the soundscape, it will become difficult to have interaction within the crowd. It may be that it is more relevant to create units ‘humans’ of which the crowd unit will be an emergent characteristic just as in the real world. That would enable us to define the sounds of cutlery, pouring liquid and clothes, as units that emerge from interaction.\nUnit Types Three type-pairs of opposing attributes are defined that will help to make some broad distinctions between units:\nAt the most basic level, a distinction between active and passive units can be made. An active unit produces sound as a result of an internal function, while a passive unit remains silent until it is excited by an active one. For example, a tree can be seen as a passive unit that may be excited by an interaction with an active unit such as the wind or a door as a passive unit that can be, potentially, excited by the action of a human (active unit) opening the door.\nA unit may be static or mobile according to its potential for motion2. Although at first it may seem that the active and passive types would also describe whether something is static or mobile, this is not necessarily the case. A passive unit may be set to motion by an interaction, which would require to describe it (and implement it) as a mobile unit. In the same way an active object can be imagined, that does not include motion in its behaviour.\nAlthough many sound events are clearly part of a unit and can thus be described as simple, there are cases in which a complex unit is comprised from what appear to be separate sounds. The sound of a walking human for example, is clearly a different sound from the voice of the person. Nevertheless, it makes more sense to consider them as belonging to the same underlying unit connected by the knowledge of what comprises a human3.\nObjects and Agents Another useful distinction concerning units is that between objects and agents. An agent to use Mat Buckland’s definition is ‘a system situated within and as part of an environment that senses that environment and acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future’ 4. An object on the other hand could be defined as anything that is not an agent. The buzzing electric light trap is better conceived as an object, while the moth attracted by light objects (pursuit of its own agenda) and moving towards them (acts on its environment), as an agent.\nWhile an object can be almost anything, an agent implies some kind of an organism or at least something controlled by an organism5. Consequently certain distinct features of such a unit can be identified that might help in the attempt for its construction:\nA large part of an organism’s sound can be referred to as function sounds. The term describes any sound that is not intentional, or is the result of an intention that does not primarily attempt to produce the sound. These can be further distinguished to integral sounds, which are part of the units ongoing function (breathing, heartbeat etc.), motion sounds that are the result of the units motion in space, and state dependant events that depend on the changing state of the unit (a ‘sick’ stage may add the sounds ‘sneeze’ and ‘cough’ in the function sounds). Motion sounds can be further categorised into gait (produced by legged animals), crawl, fly, swim and mechanical.\nThe second category is that of conscious sounds, and describes any sound that is produced as the direct outcome of an intention. The function of a conscious sound is mostly for the purposes of communication. Indeed, it is hard to think of other uses apart from maybe the production of music which could be considered as a form of communication as well.\nEffects When two objects collide or otherwise interact, the sound result of their interaction is in most cases better described as a separate single unit. This effect6 unit is particularly useful, because it also allows as to separate the reactions of the environment from the units. In this way, the interaction of two units may produce a third unit ‘fire’ instead of considering fire as something happening to a unit. The ‘fire’ unit can then interact further with the other units to produce in them a relevant change of state7.\nAtmospheres An atmosphere8 refers to a background sound that acts as the backdrop for the actual units. The distinction of an atmosphere from a unit is an ambiguous one. For example in a forest environment the atmosphere may be a collective recording of insects, birds and other animals. Moving closer to the background should transform it into a detailed environment where all these are individual units. In an ideal environment there is no background, there are just units further away, waiting to be focused on. Realistically though, such an environment would take an enormous amount of time and processing power in order to be implemented and this is why the concept of an atmosphere is useful9.\nCutscenes In a computer game a cutscene is a fixed, non-interactive animation sequence, reproduced at a particular point in the game to push the narrative forward 10. It is directly evident that the same concept can be applied in a virtual sound environment, providing an opportunity to accomplish things that are not possible in real time. For example a generative composition in which one of the dynamic elements activates a pre-composed passage. This would allow for the use of sounds with the quality and craftsmanship that can be found in the composed works of acousmatic music.\nApart from using a cutscene as a static sequence though, there is the possibility of creating a combination of defined and generative elements that are placed in a timeline. Multiple timelines containing any number of scheduling mechanisms can then be used, allowing for the interactions to continue while a cutscene is reproduced. A unit may collide and activate an effect that is in essence a cutscene. As the cutscene unravels at the point of collision, the unit may be still in motion, continuously interacting with its environment. Because it may seem that a cutscene could actually be described as a unit, it is important to clarify that it is not interacting with the environment after it is set in motion. It is merely an ornamental, to a certain point, pre-composed sound, no matter how flexible its internal form may be.\nInteraction Having described the main building blocks of a sound environment, what remains is to consider the ways these blocks interact with each other. In practice, due to technical limitations interactions are usually allowed for only a portion of the units. It will also be necessary to enable this interaction with only a small part of the environment, a choice made according to the desired perceptual result.\nTwo types of interaction have been identified:\nCausal interaction describes the physical interaction of units in space. It is described as the forces (and their results) created when two units are found to occupy the same space at the same time, and in accordance with the object’s mass, velocity and any other relevant attribute.\nPerceptual interaction is interaction of the world with the ‘senses’ of a unit. This interaction includes the observation of its environment that may result to a consequent change of behaviour, as well as the related possibility of communication between units.\nTheories and Techniques Relevant to the Implementation of the Structure Although there is not a single way to realise the concepts described in the first part of the article, there exist a useful collection of approaches that arise from past attempts. This is particularly true concerning the problems that have a direct counterpart in the virtual worlds of computer games and for which there exists an enormous amount of material. The following sections will introduce main issues concerning the creation of compelling virtual sound environments. The goal of this section is to present a concise list of relevant methods and techniques as well as a clear description of some basic but essential requirements, that seem to be ignored in existing models of virtual sound environments. A purely sound based implementation of some of the methods can be found in the GameLoop library for the SuperCollider programming language, created by the author 11.\nThe Modelling of Space The first issue is to choose a technique that would allow for the accurate rendering of space. This is really the most basic requirement in order to start considering any further techniques. If spatial motion is not perceived, the translation of techniques from the spatially clear world of visuals will not function. A system where the rules of space have been formalised and are producing consistent results needs therefore to be used. This is possible and beneficial to be realised as a virtual model of a space where the composer/programmer is dealing with indefinite coordinates, whether they are polar or Cartesian. The algorithm should take care (apart from the actual spatialisation method) of amplitude attenuation, low-pass filtering, doppler shifts, reverberation etc. according to the unit’s position and motion.\nIn addition to the use of a modelling algorithm, however, attention should be directed towards the fact that the sound image is not as well defined as a visual one. A sound object’s position in many cases depends on the recognition of its source and can thus produce inconsistent spatial imagery. For example, without any reverberation clues, to distinguish between high frequency components that are absent because of air absorption and thus distance and the sound’s natural spectrum the brain has to draw from previous experience and contextual ques. The coherence of the virtual space depends not only on the convincing placement and motion of its individual elements, but on the way their images interact with each other. A theory that explains these ambiguities has not been formulated and consequently their resolution depends on the judgement of the composer.\nUnit Construction Although the construction of a unit naturally depends on the particular case, there are some characteristics that are common to all units. Apart from the common space algorithms that have already been discussed, a unit’s internal morphology should in most cases be affected by its motion in space and react accordingly to the interaction with its environment. The most general of these is the way an agent’s speed reflects on its sound. In most cases an increase in speed comes about by an internal increase of energy in the system and has a sonic manifestation. The gait of an animal increases in frequency as it speeds up and the same happens for a flying, swimming or even mechanical agent. If a convincing illusion of such a sound image is desired, it needs to take into account these changes. On a more particular level the unit should implement sound outcomes reflecting changes in its state resulting from external or internal factors. For example, these changes typically involve a method for the unit’s response in case of collision.\nApart from these general cases, unit construction has infinite possibilities and is closely related to sound design. Since in the present article the discussion concerns dynamic sound environments, a unit’s sound design is better conceived in terms of procedural audio. Andy Farnell defines procedural audio as:\nProcedural audio is sound qua process, as opposed to sound qua product… [a] non-linear, often synthetic sound, created in real time according to a set of programmatic rules and live input. 12\nIn essence procedural sound is sound that is not predefined, but is constructed using external data according to current circumstances. Although Farnell emphasises the synthesised sound approach due to its relevance to the processing requirements in video games, any sound design technique, including the use of prerecorded samples can be used for the construction of a unit13. The challenge in using samples, (as described in Farnell’s paper) is to achieve an illusion of variation from the static nature of sound recordings and resolve spatial conflicts (like different reverberation characteristics).\nTo return briefly to the discussion of types of units, regardless of the technique used a continuum can be identified. On one end are found abstract units that do not have any real-world associations. Progressing on the continuum, associations become more apparent but are not in harmony with real world experience. These are surreal units, characteristic of the worlds of acousmatic composition. Finally at the end of the continuum are found the attempts at the modelling of real world sounds that can be referred to as natural units. The last category is of particular relevance to video games, but it can instruct the construction of any type of unit.\nSteering Behaviours and Finite State Machines Once an active and mobile unit is constructed, the next step is to imbue in it an illusion of purposeful behaviour. Although more ’traditional’ techniques, such as envelopes and low frequency oscillators, can of course still be part of the design, some aspects of the sound need to be attached to its behaviour in order for the sound to appear more ‘alive’. The most important of these parameters is that of spatial motion. Since an event’s motion is now described by its current velocity and its interaction with external forces, it is not appropriate in most situations to bypass it and use trajectories (aka. envelopes). Instead, the rich repository of techniques used in game programming, provides a collection of motion algorithms known as steering behaviours.\nThe idea of steering behaviours can be traced back to Craig Reynolds’ 1987 paper ‘Flocks, Herds, and Schools: A Distributed Behavioral Model’ 14. In this paper Reynolds describes an algorithm that produces a beautiful simulation of flocks. Reynolds elaborates on this concept in the paper ‘Steering behaviors for autonomous characters’15 where he presents a comprehensive collection of behaviours ranging from casual motion around a space to complex group formations. In his own words, the ‘paper presents solutions for one requirement of autonomous characters in animation and games: the ability to navigate around their world in a life-like and improvisational manner’.\nAfter a number of motion types have been convincingly implemented, a mechanism for the combination of behaviours by one unit should be created as well as the ability to choose the relevant behaviour according to circumstances. The latter is usually implemented using finite state machines that allow a unit to alternate between a number of states and thus modify its behaviour.\nInteraction The authors first attempts at the implementation of an interactive world consisted of creating individual units with mechanisms for exchanging information. The assumption was that the world is not an independent object, but emerges from the combination of units. The complexity, however, of having every unit sharing data with every other unit is infinitely reduced by the use of a world object that acts as a container for the units. The ‘world’ is then responsible for keeping track of causal interactions between units, as well as making it easy to simulate perception for a unit since a it can ask the container for the units in its field of vision (senses) and use the data accordingly.\nThere also exists a huge amount of research related to the resolution of the computational issues regarding collision detection 16. These techniques need to be researched for any serious implementation of a virtual environment to take place. Most of them use a spatial index to allow for a reduction of the number of checks between objects, that in turn enables an increase in the number of elements that can simultaneously exist and interact at any given time.\nUnits in Time A central problem in creating a virtual sound environment is how to determine when a sound should occur. Approaching the problem as an attempt to model the real sound environment, an attempt to define all the objects that are currently in it could be made, independent of whether they emit a sound or not. The sound events will then be produced in accord with their interactions and behaviours. As a less exact simulation, sound events can be constructed as individual units, with conditions for the time they will ’live’. This is an approach that is closer to the thinking of acousmatic composition, which tends to deal with such individual sound events and imply the underlying unit through their placement in time.\nThe decision of which kind of approach to choose depends on the particular situation, and a combination of the two is likely to be the most effective solution in many cases. Just as in video games, there are parts of the environment that are decorative and parts that are interactive. For the decorative ones, good results can be achieved by the placement of events in time using probability distributions or multiple “for loops” with random trigger times. In the majority of the sounds in a real world environments, interactions are not perceived and the sound causes remain unknown17. Finally units can be arranged in defined or partly-defined sequences, as described in the section on cutscenes.\nConclusion Although the construction of virtual sound environments is still dominated by a visual focus, it is possible that such ‘worlds’ can be implemented in a perceptually convincing way. This article presented a few concepts to assist in the design of such environments by identifying their main building blocks. In addition, it indicated relations to problems known from video game design and their respective solutions. These techniques should be examined in relation to their application in the sound domain, with strict evaluation of their perceived clarity. The level of clarity that can be achieved will define the range of possible applications for such a system. In addition to its use as an artistic tool for installation art and acousmatic composition, it is hoped to prove useful to the creation of a navigable sound world and in extension to the creation of audio games and other interactive applications with environments as compelling as their visual counterparts.\nB. Truax, The world soundscape project’s handbook for acoustic ecology. Vancouver: ARC Publications, 1978. ↩︎\nA similar distinction can be seen in almost every game implementation, in the creation of mobile and static entities. ↩︎\nA complex unit is a higher level abstraction closer to what is described by Bregman as schema based segregation 18, in contrast to more low level units that depend more on sequential and simultaneous stream segregation. ↩︎\nM. Buckland, Programming game AI by example. Texas: Wordware Publishing, 2004. ↩︎\nThis is not strictly true as it has already been observed that certain natural phenomena such as the wind could be implemented as agents. ↩︎\nThe concept was inspired by a, slightly different, definition in the book Introduction to Game Development 10. ↩︎\nThe concept of states with be discussed later on in the article. ↩︎\nThe concept was inspired by the use of the term by Chris Watson in his talk at the University of Ulster (Art College), Belfast, on the 18th of June 2010. ↩︎\nIn any case techniques can be used that would turn an atmosphere into a detailed environment when the user is approaching. ↩︎\nS. Rabin, Introduction to game development. Rockland: Charles River Media, 2005. ↩︎ ↩︎\nD. Athinaios, Gameloop supercollider library. (Available at https://github.com/dathinaios/gameloop) ↩︎\nA. Farnell, An introduction to procedural audio and its application in computer games, 2007. ↩︎\nUnits that use live input from their environment could also be considered. ↩︎\nC. Reynolds,Flocks, herds and schools: A distributed behavioral model, in Proceedings of the 14th annual conference on Computer Graphics and Interactive Techniques, N. York, 1987, pp. 25–34. ↩︎\nC. Reynolds, Steering behaviors for autonomous characters in Game Developers Conference, San Jose California; Miller Freeman Game Group, 1999, pp. 763–782. ↩︎\nC. Ericson, Real-time collision detection. San Francisco: Morgan Kaufmann Publishers, 2004. ↩︎\nIn such cases sounds can also be excluded from collision detection freeing up valuable resources. ↩︎\nA. Bregman, Auditory scene analysis: the perceptual organization of sound. N. York: The MIT Press, 1994. ↩︎\n","wordCount":"3858","inLanguage":"en","datePublished":"2014-09-05T00:00:00Z","dateModified":"2014-09-05T00:00:00Z","author":{"@type":"Person","name":"Dionysis Athinaios"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dathinaios.github.io/posts/virtual_sound_environments/"},"publisher":{"@type":"Organization","name":"Dionysis Athinaios","logo":{"@type":"ImageObject","url":"https://dathinaios.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove('dark')</script><header class=header><nav class=nav><div class=logo><a href=https://dathinaios.github.io accesskey=h title="Dionysis Athinaios (Alt + H)">Dionysis Athinaios</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dathinaios.github.io/about/ title=About><span>About</span></a></li><li><a href=https://dathinaios.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://dathinaios.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://github.com/dathinaios title=GitHub><span>GitHub</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dathinaios.github.io>Home</a>&nbsp;»&nbsp;<a href=https://dathinaios.github.io/posts/>Posts</a></div><h1 class=post-title>Virtual Sound Environments: Concepts and Methods</h1><div class=post-meta><span title='2014-09-05 00:00:00 +0000 UTC'>September 5, 2014</span>&nbsp;·&nbsp;Dionysis Athinaios</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#the-main-building-blocks aria-label="The Main Building Blocks">The Main Building Blocks</a><ul><li><a href=#units aria-label=Units>Units</a><ul><li><a href=#unit-types aria-label="Unit Types">Unit Types</a></li><li><a href=#objects-and-agents aria-label="Objects and Agents">Objects and Agents</a></li><li><a href=#effects aria-label=Effects>Effects</a></li></ul></li><li><a href=#atmospheres aria-label=Atmospheres>Atmospheres</a></li><li><a href=#cutscenes aria-label=Cutscenes>Cutscenes</a></li><li><a href=#interaction aria-label=Interaction>Interaction</a></li></ul></li><li><a href=#theories-and-techniques-relevant-to-the-implementation-of-the-structure aria-label="Theories and Techniques Relevant to the Implementation of the Structure">Theories and Techniques Relevant to the Implementation of the Structure</a><ul><li><a href=#the-modelling-of-space aria-label="The Modelling of Space">The Modelling of Space</a></li><li><a href=#unit-construction aria-label="Unit Construction">Unit Construction</a></li><li><a href=#steering-behaviours-and-finite-state-machines aria-label="Steering Behaviours and Finite State Machines">Steering Behaviours and Finite State Machines</a></li><li><a href=#interaction-1 aria-label=Interaction>Interaction</a></li></ul></li><li><a href=#units-in-time aria-label="Units in Time">Units in Time</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>This article presents concepts that are useful for the creation of virtual
sound environments. Although in most cases it uses examples from the
real world, the concepts should not be viewed as an attempt to
categorise, analyse or model real soundscapes. Throughout, the focus
is on the creation of perceptually clear and dynamic virtual sound
environments that allude to the experience of sound spaces and events,
much in the same way that this is done in video games.</p><p>In a video game, the creators do not attempt in most cases to recreate a
real space. Their target is to create a self contained world, with its
own rules, that would allow the player to engage and interact with its
elements. There will be no consideration of the cultural and personal
ramifications of the term <em>soundscape</em> that have been proposed during
the past years <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Instead a collection of
concepts that allows for multiple interpretations of a sound environment
depending on the intended outcome will be presented, leaving the
decisions regarding content and its related meaning to the judgement of
the artist or the analytical approach of the researcher accordingly.</p><p>The <strong>unit</strong> is defined as the basic element for the construction of a
virtual sound environment and its several types, followed by
considerations of some complimentary elements and a discussion
concerning interaction. The second part of the article will touch on the
basics of unit creation, as well as their behaviour and interaction,
presenting a number of techniques that are essential for every
implementation.</p><h2 id=the-main-building-blocks>The Main Building Blocks<a hidden class=anchor aria-hidden=true href=#the-main-building-blocks>#</a></h2><h3 id=units>Units<a hidden class=anchor aria-hidden=true href=#units>#</a></h3><p>A unit is a sound that can be mentally isolated from the environment for
consideration and that perceptually has the capacity to stand as an
autonomous entity. It is important to understand that the concept of a
unit is based on a subjective interpretation of the world. Is the unit
the swarm of bees? The individual bee? A cell in the bee&rsquo;s body? Should
the flowers be included as well? The boundaries of the world are judged
by the human mind and can have multiple, sometimes conflicting, but
nevertheless valid interpretations.</p><p>So how can a decision for what a unit is be made? On the simplest level,
a unit seems to be described by a word. The fact that there is a symbol
assigned to it is a clear indication that it has been identified as a
unit. Taking a restaurant as an example there the following sounds can
be heard: people talking, the sound of cutlery, the waiters footsteps as
they go about their work, the gentle sound of their clothing as people
move, music playing in the background. Occasionally a more distinct
event appears such as the sound of pouring liquid in a glass, the &lsquo;plop&rsquo;
of a cork being removed or the sound of someone laughing.</p><p>This description could be chosen as a guide and the composition of a
soundscape commence but that would not necessarily be the best strategy.
In choosing the units that will be used, the amount of interactivity
that the environment has is chosen as well. If a unit &lsquo;crowd&rsquo; is placed
in the background of the soundscape, it will become difficult to have
interaction within the crowd. It may be that it is more relevant to
create units &lsquo;humans&rsquo; of which the crowd unit will be an emergent
characteristic just as in the real world. That would enable us to define
the sounds of cutlery, pouring liquid and clothes, as units that emerge
from interaction.</p><h4 id=unit-types>Unit Types<a hidden class=anchor aria-hidden=true href=#unit-types>#</a></h4><p>Three type-pairs of opposing attributes are defined that will help to
make some broad distinctions between units:</p><ol><li><p>At the most basic level, a distinction between <strong>active</strong> and
<strong>passive</strong> units can be made. An active unit produces sound as a
result of an internal function, while a passive unit remains silent
until it is excited by an active one. For example, a tree can be
seen as a passive unit that may be excited by an interaction with an
active unit such as the wind or a door as a passive unit that can
be, potentially, excited by the action of a human (active unit)
opening the door.</p></li><li><p>A unit may be <strong>static</strong> or <strong>mobile</strong> according to its potential for
motion<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. Although at first it may seem that the active and
passive types would also describe whether something is static or
mobile, this is not necessarily the case. A passive unit may be set
to motion by an interaction, which would require to describe it (and
implement it) as a mobile unit. In the same way an active object can
be imagined, that does not include motion in its behaviour.</p></li><li><p>Although many sound events are clearly part of a unit and can thus
be described as <strong>simple</strong>, there are cases in which a <strong>complex</strong> unit
is comprised from what appear to be separate sounds. The sound of a
walking human for example, is clearly a different sound from the
voice of the person. Nevertheless, it makes more sense to consider
them as belonging to the same underlying unit connected by the
knowledge of what comprises a human<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p></li></ol><h4 id=objects-and-agents>Objects and Agents<a hidden class=anchor aria-hidden=true href=#objects-and-agents>#</a></h4><p>Another useful distinction concerning units is that between <strong>objects</strong>
and <strong>agents</strong>. An agent to use Mat Buckland&rsquo;s definition is &lsquo;a system
situated within and as part of an environment that senses that
environment and acts on it, over time, in pursuit of its own agenda and
so as to effect what it senses in the future&rsquo;
<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>. An object on the other hand could be
defined as anything that is not an agent. The buzzing electric light
trap is better conceived as an object, while the moth attracted by light
objects (pursuit of its own agenda) and moving towards them (acts on its
environment), as an agent.</p><p>While an object can be almost anything, an agent implies some kind of an
organism or at least something controlled by an organism<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>.
Consequently certain distinct features of such a unit can be identified
that might help in the attempt for its construction:</p><ol><li><p>A large part of an organism&rsquo;s sound can be referred to as <strong>function
sounds</strong>. The term describes any sound that is not intentional, or is
the result of an intention that does not primarily attempt to
produce the sound. These can be further distinguished to <strong>integral</strong>
sounds, which are part of the units ongoing function (breathing,
heartbeat etc.), <strong>motion</strong> sounds that are the result of the units
motion in space, and <strong>state dependant</strong> events that depend on the
changing state of the unit (a &lsquo;sick&rsquo; stage may add the sounds
&lsquo;sneeze&rsquo; and &lsquo;cough&rsquo; in the function sounds). Motion sounds can be
further categorised into <strong>gait</strong> (produced by legged animals),
<strong>crawl</strong>, <strong>fly</strong>, <strong>swim</strong> and <strong>mechanical</strong>.</p></li><li><p>The second category is that of <strong>conscious sounds</strong>, and describes any
sound that is produced as the direct outcome of an intention. The
function of a conscious sound is mostly for the purposes of
communication. Indeed, it is hard to think of other uses apart from
maybe the production of music which could be considered as a form of
communication as well.</p></li></ol><h4 id=effects>Effects<a hidden class=anchor aria-hidden=true href=#effects>#</a></h4><p>When two objects collide or otherwise interact, the sound result of
their interaction is in most cases better described as a separate single
unit. This <strong>effect</strong><sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> unit is particularly useful, because it also
allows as to separate the reactions of the environment from the units.
In this way, the interaction of two units may produce a third unit
&lsquo;fire&rsquo; instead of considering fire as something happening to a unit. The
&lsquo;fire&rsquo; unit can then interact further with the other units to produce in
them a relevant change of state<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>.</p><h3 id=atmospheres>Atmospheres<a hidden class=anchor aria-hidden=true href=#atmospheres>#</a></h3><p>An <strong>atmosphere</strong><sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup> refers to a background sound that acts as the
backdrop for the actual units. The distinction of an atmosphere from a
unit is an ambiguous one. For example in a forest environment the
atmosphere may be a collective recording of insects, birds and other
animals. Moving closer to the background should transform it into a
detailed environment where all these are individual units. In an ideal
environment there is no background, there are just units further away,
waiting to be focused on. Realistically though, such an environment
would take an enormous amount of time and processing power in order to
be implemented and this is why the concept of an atmosphere is
useful<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>.</p><h3 id=cutscenes>Cutscenes<a hidden class=anchor aria-hidden=true href=#cutscenes>#</a></h3><p>In a computer game a cutscene is a fixed, non-interactive animation
sequence, reproduced at a particular point in the game to push the
narrative forward <sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>. It is directly
evident that the same concept can be applied in a virtual sound
environment, providing an opportunity to accomplish things that are not
possible in real time. For example a generative composition in which one
of the dynamic elements activates a pre-composed passage. This would
allow for the use of sounds with the quality and craftsmanship that can
be found in the composed works of acousmatic music.</p><p>Apart from using a cutscene as a static sequence though, there is the
possibility of creating a combination of defined and generative elements
that are placed in a timeline. Multiple timelines containing any number
of scheduling mechanisms can then be used, allowing for the interactions
to continue while a cutscene is reproduced. A unit may collide and
activate an effect that is in essence a cutscene. As the cutscene
unravels at the point of collision, the unit may be still in motion,
continuously interacting with its environment. Because it may seem that
a cutscene could actually be described as a unit, it is important to
clarify that it is not interacting with the environment after it is set
in motion. It is merely an ornamental, to a certain point, pre-composed
sound, no matter how flexible its internal form may be.</p><h3 id=interaction>Interaction<a hidden class=anchor aria-hidden=true href=#interaction>#</a></h3><p>Having described the main building blocks of a sound environment, what
remains is to consider the ways these blocks interact with each other.
In practice, due to technical limitations interactions are usually
allowed for only a portion of the units. It will also be necessary to
enable this interaction with only a small part of the environment, a
choice made according to the desired perceptual result.</p><p>Two types of interaction have been identified:</p><ol><li><p><strong>Causal interaction</strong> describes the physical interaction of units in
space. It is described as the forces (and their results) created
when two units are found to occupy the same space at the same time,
and in accordance with the object&rsquo;s mass, velocity and any other
relevant attribute.</p></li><li><p><strong>Perceptual interaction</strong> is interaction of the world with the
&lsquo;senses&rsquo; of a unit. This interaction includes the observation of its
environment that may result to a consequent change of behaviour, as
well as the related possibility of communication between units.</p></li></ol><h2 id=theories-and-techniques-relevant-to-the-implementation-of-the-structure>Theories and Techniques Relevant to the Implementation of the Structure<a hidden class=anchor aria-hidden=true href=#theories-and-techniques-relevant-to-the-implementation-of-the-structure>#</a></h2><p>Although there is not a single way to realise the concepts described in
the first part of the article, there exist a useful collection of
approaches that arise from past attempts. This is particularly true
concerning the problems that have a direct counterpart in the virtual
worlds of computer games and for which there exists an enormous amount
of material. The following sections will introduce main issues
concerning the creation of compelling virtual sound environments. The
goal of this section is to present a concise list of relevant methods
and techniques as well as a clear description of some basic but
essential requirements, that seem to be ignored in existing models of
virtual sound environments. A purely sound based implementation of some
of the methods can be found in the GameLoop library for the
SuperCollider programming language, created by the author
<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>.</p><h3 id=the-modelling-of-space>The Modelling of Space<a hidden class=anchor aria-hidden=true href=#the-modelling-of-space>#</a></h3><p>The first issue is to choose a technique that would allow for the
accurate rendering of space. This is really the most basic requirement
in order to start considering any further techniques. If spatial motion
is not perceived, the translation of techniques from the spatially clear
world of visuals will not function. A system where the rules of space
have been formalised and are producing consistent results needs
therefore to be used. This is possible and beneficial to be realised as
a virtual model of a space where the composer/programmer is dealing with
indefinite coordinates, whether they are polar or Cartesian. The
algorithm should take care (apart from the actual spatialisation method)
of amplitude attenuation, low-pass filtering, doppler shifts,
reverberation etc. according to the unit&rsquo;s position and motion.</p><p>In addition to the use of a modelling algorithm, however, attention
should be directed towards the fact that the sound image is not as well
defined as a visual one. A sound object&rsquo;s position in many cases depends
on the recognition of its source and can thus produce inconsistent
spatial imagery. For example, without any reverberation clues, to
distinguish between high frequency components that are absent because of
air absorption and thus distance and the sound&rsquo;s natural spectrum the
brain has to draw from previous experience and contextual ques. The
coherence of the virtual space depends not only on the convincing
placement and motion of its individual elements, but on the way their
images interact with each other. A theory that explains these
ambiguities has not been formulated and consequently their resolution
depends on the judgement of the composer.</p><h3 id=unit-construction>Unit Construction<a hidden class=anchor aria-hidden=true href=#unit-construction>#</a></h3><p>Although the construction of a unit naturally depends on the particular
case, there are some characteristics that are common to all units. Apart
from the common space algorithms that have already been discussed, a
unit&rsquo;s internal morphology should in most cases be affected by its
motion in space and react accordingly to the interaction with its
environment. The most general of these is the way an agent&rsquo;s speed
reflects on its sound. In most cases an increase in speed comes about by
an internal increase of energy in the system and has a sonic
manifestation. The gait of an animal increases in frequency as it speeds
up and the same happens for a flying, swimming or even mechanical agent.
If a convincing illusion of such a sound image is desired, it needs to
take into account these changes. On a more particular level the unit
should implement sound outcomes reflecting changes in its state
resulting from external or internal factors. For example, these changes
typically involve a method for the unit&rsquo;s response in case of collision.</p><p>Apart from these general cases, unit construction has infinite
possibilities and is closely related to sound design. Since in the
present article the discussion concerns dynamic sound environments, a
unit&rsquo;s sound design is better conceived in terms of procedural audio.
Andy Farnell defines procedural audio as:</p><blockquote><p><em>Procedural audio is sound qua process, as opposed to sound qua
product&mldr; [a] non-linear, often synthetic sound, created in real
time according to a set of programmatic rules and live input.</em>
<sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup></p></blockquote><p>In essence procedural sound is sound that is not predefined, but is
constructed using external data according to current circumstances.
Although Farnell emphasises the synthesised sound approach due to its
relevance to the processing requirements in video games, any sound
design technique, including the use of prerecorded samples can be used
for the construction of a unit<sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>. The challenge in using samples, (as
described in Farnell&rsquo;s paper) is to achieve an illusion of variation
from the static nature of sound recordings and resolve spatial conflicts
(like different reverberation characteristics).</p><p>To return briefly to the discussion of types of units, regardless of the
technique used a continuum can be identified. On one end are found
<strong>abstract units</strong> that do not have any real-world associations.
Progressing on the continuum, associations become more apparent but are
not in harmony with real world experience. These are <strong>surreal units</strong>,
characteristic of the worlds of acousmatic composition. Finally at the
end of the continuum are found the attempts at the modelling of real
world sounds that can be referred to as <strong>natural units</strong>. The last
category is of particular relevance to video games, but it can instruct
the construction of any type of unit.</p><h3 id=steering-behaviours-and-finite-state-machines>Steering Behaviours and Finite State Machines<a hidden class=anchor aria-hidden=true href=#steering-behaviours-and-finite-state-machines>#</a></h3><p>Once an active and mobile unit is constructed, the next step is to imbue
in it an illusion of purposeful behaviour. Although more &rsquo;traditional&rsquo;
techniques, such as envelopes and low frequency oscillators, can of
course still be part of the design, some aspects of the sound need to be
attached to its behaviour in order for the sound to appear more &lsquo;alive&rsquo;.
The most important of these parameters is that of spatial motion. Since
an event&rsquo;s motion is now described by its current velocity and its
interaction with external forces, it is not appropriate in most
situations to bypass it and use trajectories (aka. envelopes). Instead,
the rich repository of techniques used in game programming, provides a
collection of motion algorithms known as steering behaviours.</p><p>The idea of steering behaviours can be traced back to Craig Reynolds&rsquo;
1987 paper &lsquo;Flocks, Herds, and Schools: A Distributed Behavioral Model&rsquo;
<sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>. In this paper Reynolds describes an
algorithm that produces a beautiful simulation of flocks. Reynolds
elaborates on this concept in the paper &lsquo;Steering behaviors for
autonomous characters&rsquo;<sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup> where he
presents a comprehensive collection of behaviours ranging from casual
motion around a space to complex group formations. In his own words, the
<em>&lsquo;paper presents solutions for one requirement of autonomous characters
in animation and games: the ability to navigate around their world in a
life-like and improvisational manner&rsquo;</em>.</p><p>After a number of motion types have been convincingly implemented, a
mechanism for the combination of behaviours by one unit should be
created as well as the ability to choose the relevant behaviour
according to circumstances. The latter is usually implemented using
<strong>finite state machines</strong> that allow a unit to alternate between a number
of states and thus modify its behaviour.</p><h3 id=interaction-1>Interaction<a hidden class=anchor aria-hidden=true href=#interaction-1>#</a></h3><p>The authors first attempts at the implementation of an interactive world
consisted of creating individual units with mechanisms for exchanging
information. The assumption was that the world is not an independent
object, but emerges from the combination of units. The complexity,
however, of having every unit sharing data with every other unit is
infinitely reduced by the use of a world object that acts as a container
for the units. The &lsquo;world&rsquo; is then responsible for keeping track of
causal interactions between units, as well as making it easy to simulate
perception for a unit since a it can ask the container for the units in
its field of vision (senses) and use the data accordingly.</p><p>There also exists a huge amount of research related to the resolution of
the computational issues regarding collision detection
<sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup>. These techniques need to be
researched for any serious implementation of a virtual environment to
take place. Most of them use a spatial index to allow for a reduction of
the number of checks between objects, that in turn enables an increase
in the number of elements that can simultaneously exist and interact at
any given time.</p><h2 id=units-in-time>Units in Time<a hidden class=anchor aria-hidden=true href=#units-in-time>#</a></h2><p>A central problem in creating a virtual sound environment is how to
determine when a sound should occur. Approaching the problem as an
attempt to model the real sound environment, an attempt to define all
the objects that are currently in it could be made, independent of
whether they emit a sound or not. The sound events will then be produced
in accord with their interactions and behaviours. As a less exact
simulation, sound events can be constructed as individual units, with
conditions for the time they will &rsquo;live&rsquo;. This is an approach that is
closer to the thinking of acousmatic composition, which tends to deal
with such individual sound events and imply the underlying unit through
their placement in time.</p><p>The decision of which kind of approach to choose depends on the
particular situation, and a combination of the two is likely to be the
most effective solution in many cases. Just as in video games, there are
parts of the environment that are <strong>decorative</strong> and parts that are
<strong>interactive</strong>. For the decorative ones, good results can be achieved
by the placement of events in time using probability distributions or
multiple &ldquo;for loops&rdquo; with random trigger times. In the majority of the
sounds in a real world environments, interactions are not perceived and
the sound causes remain unknown<sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup>. Finally units can be arranged in
defined or partly-defined sequences, as described in the section on
cutscenes.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Although the construction of virtual sound environments is still
dominated by a visual focus, it is possible that such &lsquo;worlds&rsquo; can be
implemented in a perceptually convincing way. This article presented a few
concepts to assist in the design of such environments by identifying
their main building blocks. In addition, it indicated relations to
problems known from video game design and their respective solutions.
These techniques should be examined in relation to their application in
the sound domain, with strict evaluation of their perceived clarity. The
level of clarity that can be achieved will define the range of possible
applications for such a system. In addition to its use as an artistic
tool for installation art and acousmatic composition, it is hoped to
prove useful to the creation of a navigable sound world and in extension
to the creation of audio games and other interactive applications with
environments as compelling as their visual counterparts.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>B. Truax, <em>The world soundscape project’s handbook for acoustic ecology</em>. Vancouver: ARC Publications, 1978.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>A similar distinction can be seen in almost every game
implementation, in the creation of mobile and static entities.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>A complex unit is a higher level abstraction closer to what is
described by Bregman as schema based segregation
<sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>, in contrast to more low level
units that depend more on sequential and simultaneous stream
segregation.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>M. Buckland, <em>Programming game AI by example</em>. Texas: Wordware Publishing, 2004.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>This is not strictly true as it has already been observed that
certain natural phenomena such as the wind could be implemented as
agents.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>The concept was inspired by a, slightly different, definition in
the book <em>Introduction to Game Development</em>
<sup id=fnref1:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>The concept of states with be discussed later on in the article.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>The concept was inspired by the use of the term by Chris Watson in
his talk at the University of Ulster (Art College), Belfast, on the
18th of June 2010.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>In any case techniques can be used that would turn an atmosphere
into a detailed environment when the user is approaching.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>S. Rabin, <em>Introduction to game development</em>. Rockland: Charles River Media, 2005.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>D. Athinaios, <em>Gameloop supercollider library</em>. (Available at <a href=https://github.com/dathinaios/gameloop>https://github.com/dathinaios/gameloop</a>)&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>A. Farnell, <em>An introduction to procedural audio and its application in computer games</em>, 2007.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Units that use live input from their environment could also be
considered.&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>C. Reynolds,<em>Flocks, herds and schools: A distributed behavioral model</em>, in Proceedings of the 14th annual conference on Computer Graphics and Interactive Techniques, N. York, 1987, pp. 25–34.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>C. Reynolds, <em>Steering behaviors for autonomous characters</em> in Game Developers Conference, San Jose California; Miller Freeman Game Group, 1999, pp. 763–782.&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>C. Ericson, <em>Real-time collision detection</em>. San Francisco: Morgan Kaufmann Publishers, 2004.&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>In such cases sounds can also be excluded from collision detection
freeing up valuable resources.&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>A. Bregman, <em>Auditory scene analysis: the perceptual organization of sound</em>. N. York: The MIT Press, 1994.&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://dathinaios.github.io/tags/virtual-sound-environments/>virtual sound environments</a></li><li><a href=https://dathinaios.github.io/tags/audio/>audio</a></li><li><a href=https://dathinaios.github.io/tags/research/>research</a></li></ul></footer><section class="article discussion"><script>function loadComment(){let t=localStorage.getItem("pref-theme")=='dark'||localStorage.getItem("pref-theme")===null?'github-dark':'github-light',e=document.createElement('script');e.src='https://utteranc.es/client.js',e.setAttribute('repo','dathinaios/dathinaios.github.io'),e.setAttribute('issue-term','pathname'),e.setAttribute('theme',t),e.setAttribute('crossorigin','anonymous'),e.setAttribute('async',''),document.querySelector('section.article.discussion').innerHTML='',document.querySelector('section.article.discussion').appendChild(e)}loadComment(),window.matchMedia&&window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change',()=>{loadComment()})</script></section></article></main><footer class=footer><span>&copy; 2022 <a href=https://dathinaios.github.io>Dionysis Athinaios</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script><script>document.querySelectorAll('pre > code').forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement('button');e.classList.add('copy-code'),e.innerHTML='copy';function s(){e.innerHTML='copied!',setTimeout(()=>{e.innerHTML='copy'},2e3)}e.addEventListener('click',o=>{if('clipboard'in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand('copy'),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>